{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "1. Take a moment to confirm the configuration details. You can run it with default settings to get a 3 node cluster with 21GB of RAM\n",
    "2. Run the cell bellow to configure the spark cluster\n",
    "\n",
    "### Note \n",
    "\n",
    "You can change the driver and executor max memory and number of nodes by changing the following\n",
    "\n",
    "``“driverMemory”:”21G”\n",
    "“executorMemory”:”21G\n",
    "“numExecutors”:3\n",
    "``\n",
    "\n",
    "For more info, check the documentation [here][1]\n",
    "\n",
    "[1]: http://h2o-release.s3.amazonaws.com/h2o/latest_azure_doc.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.ext.h2o.announce.rest.url\": \"http://@@IPADDRESS@@:5000/flows\",\n",
    "        \"spark.jars\":\"/H2O-Sparkling-Water-files/sparkling-water-assembly-all.jar\",\n",
    "        \"spark.submit.pyFiles\":\"/H2O-Sparkling-Water-files/pySparkling.zip\",\n",
    "        \"spark.locality.wait\":\"3000\",\n",
    "        \"spark.scheduler.minRegisteredResourcesRatio\":\"1\",\n",
    "        \"spark.task.maxFailures\":\"1\",\n",
    "        \"spark.yarn.am.extraJavaOption\":\"-XX:MaxPermSize=384m\",\n",
    "        \"spark.yarn.max.executor.failures\":\"1\",\n",
    "        \"maximizeResourceAllocation\": \"true\"\n",
    "    },\n",
    "    \"driverMemory\":\"21G\",\n",
    "    \"executorMemory\":\"21G\",\n",
    "    \"numExecutors\":3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with PySparkling\n",
    "The Amazon Fine Food Reviews dataset consists of 568,454 food reviews Amazon users left up to October 2012.\n",
    "\n",
    "> This data was originally published on SNAP as part of the paper: J. McAuley and J. Leskovec. _From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews_. WWW, 2013.\n",
    "\n",
    "https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pysparkling, h2o\n",
    "import os\n",
    "os.environ[\"PYTHON_EGG_CACHE\"] = \"~/\"\n",
    "sc.addPyFile(\"wasb:///H2O-Sparkling-Water-files/pySparkling.zip\") # For Azure DataLake replace wasb with adl\n",
    "\n",
    "h2o_context = pysparkling.H2OContext.getOrCreate(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2O FLOW\n",
    "\n",
    "H2O Flow is a  interactive web-based computational user interface where you can combine code execution, text, mathematics, plots and rich media into a single document, much like Jupyter Notebooks.\n",
    "\n",
    "With H2O Flow, you can capture, rerun, annotate, present, and share your workflow. H2O Flow allows you to use H2O interactively to import files, build models, and iteratively improve them. Based on your models, you can make predictions and add rich text to create vignettes of your work - all within Flow’s browser-based environment. \n",
    "\n",
    "An H2O Flow instance is always running when H2O is started, even from R or Python. Users can use Flow in conjunction with their coding environment to evaluate model performance & scoring history easily during an training run. They can also monitor cluster & CPU usage and perform data explorations using the built-in visualizations.\n",
    "\n",
    "### Note\n",
    "Please wait for the previous cell to finish executing (and start H2O) before opening the H2O Flow page\n",
    "\n",
    "###### H2O FLOW can be found at @@FLOWURL@@\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into H2OFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is just helper function returning the path to public data file Reviews.csv ~ 300MB size\n",
    "def _locate(example_name): \n",
    "    return \"https://h2ostore.blob.core.windows.net/examples/\" + example_name \n",
    "\n",
    "DATASET = 'CReviews.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And import them into H2O\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "reviews_hf = h2o.import_file(_locate(DATASET))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_hf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data munge data with H2O API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_columns = [ \"Score\", \"Time\", \"Summary\", \"HelpfulnessNumerator\", \"HelpfulnessDenominator\" ]\n",
    "reviews_hf = reviews_hf[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_hf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine `Time` Column into Year/Month/Day/DayOfWeek/Hour columns\n",
    "In this case the `Time` column contains number of seconds from epoch. We translate it into several new columns to help algorithms to pick right pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set time zone to UTC for date manipulation\n",
    "h2o.cluster().timezone = \"Etc/UTC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def refine_time_column(data_hf, column_name):\n",
    "    data_hf[column_name] = data_hf[column_name] * 1000 # Transformation to microsecond since required by H2O API\n",
    "    data_hf[\"Day\"] = data_hf[column_name].day()\n",
    "    data_hf[\"Month\"] = data_hf[column_name].month()\n",
    "    data_hf[\"Year\"] = data_hf[column_name].year()\n",
    "    data_hf[\"DayOfWeek\"] = data_hf[column_name].dayOfWeek()\n",
    "    data_hf[\"Hour\"] = data_hf[column_name].hour()\n",
    "    \n",
    "refine_time_column(reviews_hf, \"Time\")\n",
    "reviews_hf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Munge with Spark API\n",
    "We can combine H2O data munging capabilities with Spark API\n",
    "\n",
    "### Publish H2O Frame as Spark DataFrame\n",
    "\n",
    "The created H2OContext exposes the method `as_spark_frame` which publishes an H2OFrame as Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_df = h2o_context.as_spark_frame(reviews_hf)\n",
    "reviews_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HERE is where we save the dataframe to a Hive Table\n",
    "reviews_df.createOrReplaceTempView(\"reviewstabletemp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrame API\n",
    "\n",
    "From this point we can run any Spark data munging operations including SQL.\n",
    "We can still publish the result as H2OFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avgScorePerYear = reviews_df.groupBy(\"Year\").agg({\"Score\" : \"avg\", \"*\": \"count\"}).where(\"Year is not null\").orderBy(\"Year\")\n",
    "avgScorePerYear.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avgScorePerYear.createOrReplaceTempView(\"avgscoretabletemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "show tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query the hive table and output the results on a pandas dataframe (using the -o option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql -q -n 500 -o query1\n",
    "select * from avgscoretabletemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize the results directly in Python Notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "%matplotlib inline\n",
    "\n",
    "query1.plot.bar(x=\"Year\", y = \"count(1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for modeling\n",
    "The idea is to model sentiment based on `Score` of review, `Summary` and time when the review was performed. In this case we skip all neutral reviews, but focus on positive/negative scores.\n",
    "\n",
    "Steps:\n",
    "\n",
    "  1. Select columns Score, Month, Day, DayOfWeek, Summary\n",
    "  2. Define UDF to transform score (0..5) to binary positive/negative\n",
    "  3. Use TF-IDF to vectorize summary column\n",
    "\n",
    "#### Transform the `Score` column into binary feature\n",
    "\n",
    "The score contains value (0, 5), however we are just interested in binary value - positive/negative review. We ignore neutral reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "\n",
    "def to_binary_score(col):\n",
    "    if col < 3:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"positive\"\n",
    "udf_to_binary_score = UserDefinedFunction(to_binary_score, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_df = reviews_df.withColumn(\"Score\", udf_to_binary_score(\"Score\"))\n",
    "reviews_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming textual data into numeric representation\n",
    "\n",
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Summary\", outputCol=\"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform tokens into numeric representation\n",
    "\n",
    "We use Spark `HashingTF` to represent tokens as numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF()\n",
    "hashingTF.setInputCol(\"tokens\").setOutputCol(\"tf-features\").setNumFeatures(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build IDF (Inverse Document Frequency) model\n",
    "The model scales a token frequency based on its occurence in a document and full set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = IDF()\n",
    "idf.setInputCol(\"tf-features\")\n",
    "idf.setOutputCol(\"idf-features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compose individual transformation into a Spark pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages = [tokenizer, hashingTF, idf])\n",
    "pipelineModel = pipeline.fit(reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And transform input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_reviews_df = pipelineModel.transform(reviews_df)\n",
    "#final_reviews_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to H2O Frame (materialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_columns = [\"Score\", \"HelpfulnessNumerator\", \"HelpfulnessDenominator\", \"Day\", \"Month\", \"Year\", \"DayOfWeek\", \"idf-features\"]\n",
    "final_reviews_hf = h2o_context.as_h2o_frame(final_reviews_df.select(final_columns), \"final_reviews_hf\")\n",
    "#final_reviews_hf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score and DayOfWeek columns needs to be a factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_reviews_hf[\"Score\"] = final_reviews_hf[\"Score\"].asfactor()\n",
    "final_reviews_hf[\"DayOfWeek\"] = final_reviews_hf[\"DayOfWeek\"].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and validation dataset for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splits = final_reviews_hf.split_frame(ratios=[0.75], destination_frames=[\"train\", \"valid\"], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_hf = splits[0]\n",
    "valid_hf = splits[1]\n",
    "#train_hf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_reviews_hf = None\n",
    "reviews_hf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h2o.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "### Random grid search with explicit stopping criterions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a hyper space to explore\n",
    "\n",
    "> Please feel free to play with parameters, see documentation in [H2O Python Documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/modeling.html#module-h2o.grid.grid_search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "\n",
    "hyper_params = {'activation' : [\"Rectifier\", \"TanhWithDropout\"], \n",
    "                'hidden' : [ [2,2], [10,10]],\n",
    "                'epochs' : [ 1, 2, 5]\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define stoping criterions\n",
    "\n",
    "> Modify based on your demands and requirements (time v. accuracy bound search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_criteria = {'strategy' : 'RandomDiscrete',\n",
    "                   'max_runtime_secs': 120,\n",
    "                   'stopping_rounds' : 3,\n",
    "                   'stopping_metric' : 'AUC', # AUTO, mse, logloss\n",
    "                   'stopping_tolerance': 1e-2\n",
    "                   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch Random Hyper Search\n",
    "\n",
    "> For more details look into [H2O Deep Learning documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/modeling.html#h2odeeplearningestimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models_grid = H2OGridSearch(H2ODeepLearningEstimator, hyper_params=hyper_params, search_criteria=search_criteria)\n",
    "models_grid.train(x = train_hf.col_names, y = \"Score\", \\\n",
    "                  training_frame = train_hf, \\\n",
    "                  validation_frame = valid_hf, \\\n",
    "                  variable_importances=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best model is ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models_grid.sort_by('auc', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best model details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = h2o.get_model(models_grid.sort_by('auc', False)[0][0])\n",
    "best_model.model_performance(valid_hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are most important features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model.varimp(use_pandas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations you built your first model using Azure + PySparkling and H2O!!!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
